{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell #Show all consecutive outputs\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#Removes all unnecessary warnings by Python\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "#colorbrewer2 Dark2 qualitative color table\n",
    "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
    "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
    "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
    "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
    "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
    "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
    "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['axes.color_cycle'] = dark2_colors\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['patch.facecolor'] = dark2_colors[0]\n",
    "rcParams['font.family'] = 'StixGeneral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_df=pd.read_csv(\"sentence_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(sentence_df):\n",
    "    # Load data from files\n",
    "    positive_examples = list(sentence_df[sentence_df['sentiment']==1]['Sentence'])\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(sentence_df[sentence_df['sentiment']==0]['Sentence'])\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(sentence_df):\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = load_data_and_labels(sentence_df)\n",
    "    sentences_padded = pad_sentences(sentences)\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, vocabulary, vocabulary_inv_list =load_data(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "y = y.argmax(axis=1)\n",
    "# Shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "train_len = int(len(x) * 0.9)\n",
    "x_train = x[:train_len]\n",
    "y_train = y[:train_len]\n",
    "x_test = x[train_len:]\n",
    "y_test = y[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_train shape', (2700, 81))\n",
      "('x_test shape', (300, 81))\n",
      "('vocab size', 5646)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape\",x_train.shape)\n",
    "print(\"x_test shape\",x_test.shape)\n",
    "print(\"vocab size\",len(vocabulary_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length=x_test.shape[1]\n",
    "embedding_weights=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 81)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2700 samples, validate on 300 samples\n",
      "Epoch 1/20\n",
      "3s - loss: 0.6939 - acc: 0.4889 - val_loss: 0.6927 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "4s - loss: 0.6931 - acc: 0.5041 - val_loss: 0.6877 - val_acc: 0.5700\n",
      "Epoch 3/20\n",
      "3s - loss: 0.6353 - acc: 0.6678 - val_loss: 0.5798 - val_acc: 0.7267\n",
      "Epoch 4/20\n",
      "3s - loss: 0.3933 - acc: 0.8481 - val_loss: 0.5466 - val_acc: 0.7300\n",
      "Epoch 5/20\n",
      "3s - loss: 0.2600 - acc: 0.8978 - val_loss: 0.5310 - val_acc: 0.7733\n",
      "Epoch 6/20\n",
      "3s - loss: 0.1512 - acc: 0.9459 - val_loss: 0.5833 - val_acc: 0.7667\n",
      "Epoch 7/20\n",
      "3s - loss: 0.1195 - acc: 0.9578 - val_loss: 0.5819 - val_acc: 0.7800\n",
      "Epoch 8/20\n",
      "3s - loss: 0.0837 - acc: 0.9715 - val_loss: 0.6180 - val_acc: 0.7800\n",
      "Epoch 9/20\n",
      "3s - loss: 0.0630 - acc: 0.9822 - val_loss: 0.6395 - val_acc: 0.8033\n",
      "Epoch 10/20\n",
      "3s - loss: 0.0509 - acc: 0.9837 - val_loss: 0.6819 - val_acc: 0.7933\n",
      "Epoch 11/20\n",
      "3s - loss: 0.0405 - acc: 0.9863 - val_loss: 0.7191 - val_acc: 0.7667\n",
      "Epoch 12/20\n",
      "3s - loss: 0.0382 - acc: 0.9837 - val_loss: 0.7715 - val_acc: 0.7867\n",
      "Epoch 13/20\n",
      "3s - loss: 0.0307 - acc: 0.9870 - val_loss: 0.8269 - val_acc: 0.7867\n",
      "Epoch 14/20\n",
      "3s - loss: 0.0304 - acc: 0.9889 - val_loss: 0.8888 - val_acc: 0.7767\n",
      "Epoch 15/20\n",
      "3s - loss: 0.0210 - acc: 0.9930 - val_loss: 0.9320 - val_acc: 0.7833\n",
      "Epoch 16/20\n",
      "3s - loss: 0.0162 - acc: 0.9956 - val_loss: 0.9537 - val_acc: 0.7700\n",
      "Epoch 17/20\n",
      "3s - loss: 0.0238 - acc: 0.9926 - val_loss: 0.9620 - val_acc: 0.7800\n",
      "Epoch 18/20\n",
      "3s - loss: 0.0163 - acc: 0.9948 - val_loss: 1.0385 - val_acc: 0.7700\n",
      "Epoch 19/20\n",
      "3s - loss: 0.0115 - acc: 0.9967 - val_loss: 1.0744 - val_acc: 0.7667\n",
      "Epoch 20/20\n",
      "3s - loss: 0.0113 - acc: 0.9967 - val_loss: 1.1569 - val_acc: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45411c5fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=50\n",
    "dropout_prob = (0.5, 0.8)\n",
    "num_filters =50\n",
    "hidden_dims=50\n",
    "filter_sizes = (3,4,5)\n",
    "#training params\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "input_shape = (sequence_length,)\n",
    "model_input = Input(shape=input_shape)\n",
    "z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "#convolutional\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,kernel_size=sz,padding=\"valid\",activation=\"relu\",strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
